* Raft



It uses a consensus algorithm in the context of Replicated State Machines. In this context, state machines on a set of nodes compute copies of the same state and continue their work even if some nodes are down. In the context of CockroachDB, the state machines are the ranges and so there is one consensus group per range. This means each nodes participates in thousands of consensus group. So they introduced a new layer they called Multiraft to handle this: it manages the communication by treating a node with all its ranges as a group. That is, each pair of nodes only needs to exchange heartbeat messages once per tick to know if the ranges on these nodes are available\cite{CRDB:multiraft}.

Raft safety properties under non-Byzantines conditions (such as network delays, partitions, packet loss, etc.) have been formally specified and proven. It also guarantees availability if a majority (n/2 + 1) of servers are operational and communicate with each other and consistency in the logs independently of the timing (delays and faulty clocks only cause availability problems). Finally, the time to complete a command is only as long as it is necessary to receive responses from the majority of the cluster.

To dive deeper into the algorithm, a few things must be explained more. At any given time, a server can be either: a /leader/, a /follower/ or a /candidate/. Only one /leader/ can be elected at any time. He sends periodic heartbeat to followers as soon as he is elected to prevent further election. The leader main roll is to accept log entries from the clients and replicate them to other servers. The /followers/ are "passive", they respond to requests from leaders and candidates. Finally, /candidates/ are used to elect a new leader.
Time is divided into /terms/ of arbitrary length. These terms are numbered with consecutive integers, increasing monotonically and all servers (ranges for CockroachDB) stores the current term. Each term begins with an election with one or more candidate try to become leader. If an election ends with a split vote, the term ends and a new one begins. This serves as a logical clock across the cluster: if two servers communicate and don't have the same term value, they take the largest one as the current. If a candidate or leader discovers this way that his term has ended, he becomes a follower. If the server receives a request with an older term, it rejects the request.
The election runs as follows: if a leader fails or is disconnected, servers know it because they don't receive the heartbeat before the election timeout, as explained before, so a new leader is elected. When a follower comes to the election timeout, it increments its current term, becomes a candidate, votes for itself and sends a RequestVote RPC to every server in the cluster. A candidate wins the election if it receives votes from a majority of servers for the same term and the followers vote for one candidate on a first-come first-served basis. If a candidate gets a signal from a leader and the term received is at least as large as the candidate current term, the candidate returns to the follower status. There is a mechanism to prevent different rounds of election due to too many followers becoming candidates at the same time: the election timeouts are chosen randomly by the servers from a fixed interval. A follower denies a candidature its log is more up-to-date than the candidate's.
An entry of a log stores a command (request), a term number and an integer identifying its position in the log when the entry was received by the leader. When receiving a new entry, the leader appends the request to its log as a new entry. It then sends the order to append the entries to the other servers to replicate it (using an AppendEntries RPC). The leader then "chooses" when an entry is committed, i.e. when an entry has replicated to a majority of servers. This operation also commits all previous entries in the leader's log, including entries from former leader. The leader maintains a nextIndex for each follower so he knows which entry it will send. RPCs are idempotent; a request that includes previously recorded log is ignored.


CockroachDB leaseholder is pretty similar to Raft's leader, but it bypasses the algorithm for reads. Indeed, if the writes have been committed, they already have achieved consensus and so this operation does not require another consensus and the leaseholder handles the read by itself. It's also worth saying that CockroachDB /attempts/ to elect a leasholder who is also a Raft group leader to optimize the speed of writes, but this means it is not mandatory\cite{CRDB:replication-layer}.

[[no_push/raft_logs.png]]
