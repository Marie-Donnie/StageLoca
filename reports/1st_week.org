#+TITLE: Report for : Un intergiciel d’une base de données NewSQL qui considère la localité de l’application cliente -- 1st week --
#+AUTHOR: Marie Delavergne



* Monday 15 January [2018-01-15 lun.]

** Installation

- Solving administrative problems (account creations, etc.)
- Working in office B226

** Preparations

- Cleaned boookmarks to be able to find them more rapidly
- Created folder and repo github for my work.
  + Added gitignore
  + Added tools
- WARNING : I can't run Vagrant without disabling secure boot


** Readings

*** Spanner

- 2013
- DB
  + scalable
  + multi-version
  + globally-distributed
  + synchronously-replicated
  + google-made
- Reshard automatically data when the amount of data or the number of servers change
- Migrate automatically data across machines and DC
- Scale up to millions of machines, hundreds of DC and trillions of datarows
- Focus: managing cross-DC replicated data
- Evolved from a versioned key-value store to a temporal multi-version db
  + semi-relational db
  + data store with a commit timestamp
  + configurable garbage collector for old versions
- Replication configurations can be dynamically controlled at a fine grain
- More Spanner features that enable Spanner to support consistent backups, MapReduce executions and atomic schema updates
  + externally consistent reads and write
  + globally-conssistent reads across DB at a given timestamp
- Timestamps reflect serialization order
- Organized as a set of zones (unit of administrative deployment)
  + one zonemaster :: assigns data to spanservers
  + (100->n*1000) spanservers
  + tablet :: data structure (similar to Bigtable tablet abstraction)
  + per-zone location proxies :: used by clients to locate the spanservers assigned to serve their data
- Spanserver
  + serve data to clients
  + responsible for between 100 and 1000 instances of tablet
  + implements a single Paxos state machine on top of each tablet
  + logs every Paxos write twice (one in the tablet's log, once in the Paxos log)
- Transaction manager
  + If the transaction involves only one Paxos group, it can bypass the TM (lock table and Paxos provide transactionality)
  + If it involves more than one Paxos group, the groups' leaders coordinate to perform a two-phase commit
    - coordinator leader chosen and coordinator slaves
- Directory (bucket) : set of contiguous keys that share a common prefix
  + allows applications to control the locality of their data by choosing keys carefully
  + directories can be moved on-the-fly for different reasons (movedir)
- Megastore used by at least 300 Google applications
- Two-phase commit too expensive for performance and availability
- Every table must have an ordered set of one or more primary-key columns
- TrueTime represents time as a TTinterval (interval with bounded time uncertainty)

| Method      | Returns                              |
| TT.now()    | TTinterval: [earliest, latest]       |
| TT.after()  | true if t has definitely passed      |
| TT.before() | true if t has definitely not arrived |

- Two-phase commit can scale to up to 50 participants
- During experimentations on availability, we can see that only kill 'hard' a leader has a real impact on the reads completed
- Related works
  + VoltDB uses NewSQL

** Meeting

- 14:15 with msimonin

*** Work
- Keystone with specific scenarii
  + tempest has more coverage (especially with the new decorator)


* Thursday 16 January [2018-01-16 mar.]

** Openstack Cockroach Dev

- Resumed vagrant launched yesterday evening
- Used ~vagrant ssh~ to jump to the deployed machine

*** Running tempest
- Jumped to tempest folder
- Used ~testr init~ and then ~testr run --parallel tempest.scenario~ to run tests from [[https://www.openstack.org/assets/presentation-media/TempestScenarioTests-20140512.pdf][Tempest Scenarios Tests]]
  + Received some errors like these :
#+BEGIN_EXAMPLE
Traceback (most recent call last):
  File "/opt/stack/neutron/neutron/tests/tempest/scenario/test_dvr.py", line 66, in test_vm_reachable_through_compute
    self._check_snat_port_connectivity()
  File "/opt/stack/neutron/neutron/tests/tempest/scenario/test_dvr.py", line 32, in _check_snat_port_connectivity
    self._check_connectivity()
  File "/opt/stack/neutron/neutron/tests/tempest/scenario/test_dvr.py", line 29, in _check_connectivity
    self.keypair['private_key'])
  File "/opt/stack/neutron/neutron/tests/tempest/scenario/base.py", line 232, in check_connectivity
    ssh_client.test_connection_auth()
  File "tempest/lib/common/ssh.py", line 207, in test_connection_auth
    connection = self._get_ssh_connection()
  File "tempest/lib/common/ssh.py", line 121, in _get_ssh_connection
    password=self.password)
tempest.lib.exceptions.SSHTimeout: Connection to the 172.24.4.12 via SSH timed out.
User: cirros, Password: None
#+END_EXAMPLE
#+BEGIN_EXAMPLE
Traceback (most recent call last):
  File "/opt/stack/neutron/neutron/tests/tempest/scenario/test_migration.py", line 200, in test_from_dvr_ha_to_dvr
    after_dvr=True, after_ha=False)
  File "/opt/stack/neutron/neutron/tests/tempest/scenario/test_migration.py", line 114, in _test_migration
    router['id'], before_dvr, before_ha)
  File "/opt/stack/neutron/neutron/tests/tempest/scenario/test_migration.py", line 80, in _wait_until_router_ports_ready
    router_id, const.DEVICE_OWNER_DVR_INTERFACE)
  File "/opt/stack/neutron/neutron/tests/tempest/scenario/test_migration.py", line 64, in _wait_until_port_ready
    timeout=300, sleep=5)
  File "/opt/stack/neutron/neutron/common/utils.py", line 697, in wait_until_true
    raise WaitTimeout("Timed out after %d seconds" % timeout)
neutron.common.utils.WaitTimeout: Timed out after 300 seconds
#+END_EXAMPLE
- Might be because only keystone is supposed to work
- Stopped the run because it was running every tests for every components
- Rerun using ~tempest run --regex tempest.api.identity~
#+BEGIN_EXAMPLE
======
Totals
======
Ran: 143 tests in 134.0000 sec.
 - Passed: 128
 - Skipped: 10
 - Expected Fail: 0
 - Unexpected Success: 0
 - Failed: 5
Sum of execute time for each test: 251.9807 sec.

==============
Worker Balance
==============
 - Worker 0 (31 tests) => 0:01:54.281398
 - Worker 1 (28 tests) => 0:01:08.869696
 - Worker 2 (14 tests) => 0:01:39.594563
 - Worker 3 (24 tests) => 0:02:00.720647
 - Worker 4 (32 tests) => 0:01:53.721540
 - Worker 5 (14 tests) => 0:00:50.875559
#+END_EXAMPLE
- Have to see the logs, which is now in a binary file (journalctl)
- ~sudo journalctl --unit devstack@keytsone.service --since -5m~ to see the logs
- I can navigate using /ERR and n to go to the next
#+BEGIN_EXAMPLE
stack@contrib-jessie:~/tempest$ sudo journalctl --unit devstack@keytsone.service --since -5m
-- Logs begin at Tue 2018-01-16 10:15:51 GMT, end at Tue 2018-01-16 10:32:08 GMT. --
stack@contrib-jessie:~/tempest$
#+END_EXAMPLE

- Modified file in keystone folder ~keystone/identity/backends/sql.py~ :
  + Search for '@', copied ~ @oslo_db_api.wrap_db_retry(retry_on_deadlock=True)~
  + Added to the method that failed 'authenticate' (l. 58)
  + After that, I restart the service using ~sudo systemctl restart devstack@keystone~
  + And check if it is correctly loaded with ~sudo systemctl status devstack@keystone~

#+BEGIN_EXAMPLE
======
Totals
======
Ran: 142 tests in 197.0000 sec.
 - Passed: 132
 - Skipped: 10
 - Expected Fail: 0
 - Unexpected Success: 0
 - Failed: 0
Sum of execute time for each test: 435.4079 sec.

==============
Worker Balance
==============
 - Worker 0 (10 tests) => 0:01:49.532543
 - Worker 1 (17 tests) => 0:02:32.577647
 - Worker 2 (22 tests) => 0:02:36.304883
 - Worker 3 (37 tests) => 0:03:10.918512
 - Worker 4 (23 tests) => 0:02:42.695828
 - Worker 5 (33 tests) => 0:02:41.024460

#+END_EXAMPLE

- When doing my PR, I discovered there was a branch called 'deadlock-retry' so I've checked it:
  + turns out there was different changes on it, and the decorator (wrapper) was used on ~_record_failed_auth~ and ~update_user~
  + but when I reran the tests on this branch I got
#+BEGIN_EXAMPLE
======
Totals
======
Ran: 132 tests in 180.0000 sec.
- Passed: 120
- Skipped: 10
- Expected Fail: 0
- Unexpected Success: 0
- Failed: 2
Sum of execute time for each test: 378.7295 sec.

==============
Worker Balance
==============
- Worker 0 (9 tests) => 0:01:30.253865
- Worker 1 (32 tests) => 0:02:36.105035
- Worker 2 (20 tests) => 0:02:24.938662
- Worker 3 (26 tests) => 0:02:29.701283
- Worker 4 (18 tests) => 0:02:09.142470
- Worker 5 (27 tests) => 0:02:47.248556
#+END_EXAMPLE

- After asking my tutor, turns out that the branch was an old one so I just made my PR to cockroachdb/pike

** Readings

*** Raft

- Goal is to have a result equivalent to Paxos, but more understandable and easier to learn
- Consensus algorithms "allow a collection of machines to work as a coherent group that can survive the failures of some of its members"
- Most implementations of consensus are based on or influenced by Paxos
- Raft uses techniques to improve understandability
  + decomposition: leader election, log replication, safety
  + state space reduction: reduce the degree of nondeterminism and ways servers can be inconsistent with each other
- New features
  + Strong leader :: stronger form of leadership
  + Leader election :: randomized timers
  + Membership changes :: two different configurations overlap when changing the set of servers in a cluster
- Safety properties formally specified and proven

**** Replicated state machine problem

- Consensus algorithms are used in the context of RSM
  + Replicated State Machines:: state machines in a collection of servers compute identical copies of the same state and continue operating even if some of the servers are down
  + It is used to solve a variety of fault tolerance problems in distributed systems

#+CAPTION: RSM architecture from the Raft article
#+NAME: fig:raft_RSM_architecture
[[images/Raft_RSM_architecture.png]]

- Each server stores a log containing a series of commands executed in order by he state machine associated
  + The commands are in the same order on each log -> same sequence executed on all state machines -> same state computed and same sequence of output (because of determinism)
- The replicated log is kept consistent by the consensus algorithm
- Properties of the algorithm
  + safety under all non-Byzantine conditions including network delays, partitions, packet loss, duplication and re-ordering
  + availability if a majority of servers are operational and communicate with each other and clients. (n/2 +1)
  + consistency of the logs independent of timing
  + speed: for most cases, a given command is completed when a majority of the cluster has responded to a single round of remote procedure calls

**** Strengths and weaknesses of Paxos

- Largely used, at least as a starting point
- Different level of Paxos:
  + single-decree Paxos:: able to reach agreement on a single decision (like a single replicated log entry)
  + multi-Paxos:: multiple instances of the former protocol to facilitate a series of decisions (such as an entire log)
- Ensures safety and liveness
- Supports changes in cluster membership
- Two major drawbacks
  + awfully  difficult to understand
  + no good foundation to make practical implementations
- As a consequence, implementations are usually extremely different from the Paxos theory, especially in terms of architecture
  + time consuming and error-prone
  + worse: since the implementations are too different, the correctness of Paxos can't be verified

**** General approach to understandability

- Raft had to:
  + provide a complete and practical foundation for system building
  + be safe under all conditions
  + be available under typical operating conditions
  + be efficient for common operations
  + be UNDERSTANDABLE
    - by a large audience
    - to make possible the development of extensions
- To make it understandable:
  + each time a choice had to be made, it was always for understandability
  + two techniques were used, as seen previously (decomposition and state space reduction)

**** Raft consensus algorithm

1. Election of a leader
   - if a leader fails or is disconnected, a new leader is elected
   - at any time each server is either:
     + leader :: only one
       - sends periodic heartbeat to followers as soon as he is elected to prevent election
     + follower :: passive, they respond to requests from leaders and candidates
     + candidate :: used to elect a new leader
   - if a follower receives no communication for a period of time (election timeout), it begins the election to choose a new leader
     + the follower increments its current term, becomes a candidate, votes for itself and sends RequestVote RPC to every servers in the cluster
       - a candidate wins the election if it receives votes from a majority of servers for the same term
       - followers vote for one candidate on a first-come first-served basis
       - if a candidate gets a signal from a leader and the term received is at least as large as the candidate current term, the candidate returns to follower
       - to prevent different rounds of elections (if many followers become candidates at the same time), election timeouts are chosen randomly from a fixed interval
2. Give responsibility for managing the replicated log to this leader
3. Leader accepts log entries from clients
4. Leader replicates them to other servers
5. Leader tells the servers when they can apply new log entries to their state machines


- Time is divided into /terms/ of arbitrary length
  + Numbered with consecutive integers; each server stores a current term (increasing monotonically)
  + each term begins with an election where one or more candidates attempt to become leader
  + if an election ends with a split vote, the term ends and a new one begins
  + Serves as a logical clock
    - if 2 servers communicate and don't have the same term value, they take the largest one
    - if a candidate or leader discovers this way that his term has ended, he becomes a follower
    - if the server receives a request with an older term, it rejects the request

- When receiving a new entry, the leader appends the command to its log as a neww entry
  + it sends the order to append the entries to the other servers to replicate the entry
- An entry of a log stores a command, a term number when the entry was received by the leader and an integer indentifying its position in the log
- The leader chooses when a entry is commited, i.e. when an entry has replicated to a majority of servers. This commit also commits all preceding entries in the leader's log (including entries from former leaders).
- Log matching properties ([[fig:raft_properties]]):
  + if 2 entries in different logs have the same index and term, they store the same command
  + if 2 entries in different logs have the same index and term, then the logs are identical in all preceding entries
- The leader maintain a nextIndex for each follower so he knows which entry it will send
- Election restriction:
  + prevents candidate from winning an election unless its log contains all committed entries
  + a follower denies a candidate vote if its own log is more up-to-date
    - if the logs have last entries with different terms, the log with the later term is more u-t-d
    - if the logs have the same terms as last entries, whichever log is longer is more u-t-d
- Raft handles followers failure by retrying indefinitely the RPCs.
  + RPCs are idempotent; a request that includes log entries already present is ignored
- Timing is crucial only for leader election
  + timing requirement: broadcastTime << electionTimeout << MTBF (average time between failures for a single server)
    - electionTimeout is the only thing chosen : usually, between 10 and 500ms
- Configuration changes comes with two phases
  + joint consensus :: combines both configurations
    - log entries are replicated to all servers in both configurations
    - any server from either configuration can be a leader
    - agreement requires separate majorities from *both* the old and new configurations
    - allows the service of clients even when configuration changes
    - leader log entry for joint consensus C_{old,new} and replicates it
    - servers only uses the latest configuration in its log, whether the entry is committed or not
    - if a leader crash (using the C_{old,new} rules), the new leader may be chosen under C_{old} or C_{old,new} (C_{new} cannot make unilateral decisions during this period)
  + once C_{old,new} has been committed, neither old nor new can make decisions without receiving approval from the other
    - only servers with the C_{old,new} entry can be leader
    - this leader create a log entry for Cnew and replicates it
  + when the new configuration has been commited, servers not in the new configuration can be shut down
  + there are no time when C_{old} or C_{new} can make unilateral decisions
  + when new servers are added, they are non-voting members (not counted for majority)
    - when they have the correct replicated log, the reconfiguration can begin
  + if the leader is not part of the new configuration, it will become follower has soon as he committed C_{new}, and will not count for votes
  + to prevent removed servers from disrupting with votes, the followers disregards the RequestVote RPC

#+CAPTION: A -not-so- condensed summary of the Raft consensus algorithm
#+NAME: fig:raft_conso_algo
[[images/Raft_conso_algo.pdf]]


#+CAPTION: Raft ensured properties
#+NAME: fig:raft_properties
[[images/Raft_properties.png]]


- Clients and log compaction can be found in the [[https://ramcloud.stanford.edu/raft.pdf][extended version of the paper]].

**** Raft evaluation

- Raft implementation contains ~2000 lines of C++ code
- available on [[http://github.com/logcabin/logcabin][Github]]
- easy to understand
- correctness uses [[fig:raft_conso_algo]] and proven in [[https://ramcloud.stanford.edu/~ongaro/thesis.pdf][proof of the State Machine Safety property]]
- performance similar to other consensus algorithms
- recommands using a conservative election timeout such as 150-300ms

**** Related work

- Lamport description of Paxos
- Elaborations of Paxos
- Systems that implement consensus algorithms (Chubby, Zookeeper and Spanner)
- Performance optimizations that can be applied to Paxos
- Viewstamped Replication from Oki and Liskov

**** Conclusion

- Better foundation for system building

** To go further

- [[http://thesecretlivesofdata.com/raft/][Animated explanation about Raft]]
  + Nothing really new but could be a good starting point if I have to explain Raft
- [[https://www.youtube.com/watch?v=YbZ3zDzDnrw&index=9&list=WL][Raft lecture video]]


* Wednesday 17 January [2018-01-17 mer.]

** Working on performance tests

- 2 starting points:
  + [[https://docs.openstack.org/performance-docs/latest/test_plans/keystone/plan.html][Keystone Performance testing]]
  + [[https://docs.openstack.org/performance-docs/latest/test_plans/db/plan.html][SQL Database Test plan]]

*** Keystone

- uses Keystone (obviously), OSprofiler and Ceilometer
  + I suspect we will use Enos so we won't have to use Ceilometer

- Test environment

- 2 types of installations:
 - single (all-in-one) installation
   + both controller and computer
 - multi-node installation
   + 4 nodes:
     - 1 compute node simulates typical OS typical activity
     - 3 controller nodes simulate activity typical for OS control plane services
       + includes 3 instances of MySQL managed by Galera cluster and memcached cluster for Keystone caching

- Preparation
  + OSprofiler library is installed on all environment nodes
     - ensure every services needed supports OSprofiler
  + needs Ceilometer for persistent profiling events storage
  + OS services must be configured to allow cross-project request profiling
    - managed on single-node by OSprofiler plugin
    - on multi-node this need to be tracked separately
  + the test cases compare the effectiveness of the use of Keystone database and cache operations
    - requires Keystone reconfiguration depending on whether the cache mechanism is used (cf cache section of keystone.conf)

#+BEGIN_EXAMPLE
[cache]
enabled = True|False
backend = oslo_cache.memcache_pool
memcache_servers = <memcached_host>:<memcached_port>[,<memcached_host>:<memcached_port>]
expiration_time = 600
#+END_EXAMPLE

**** Single node installation

- use DevStack targeted at developers and CI systems, their conf looked like this:
#+BEGIN_EXAMPLE
[[local|localrc]]
ADMIN_PASSWORD=password
DATABASE_PASSWORD=$ADMIN_PASSWORD
RABBIT_PASSWORD=$ADMIN_PASSWORD
SERVICE_PASSWORD=$ADMIN_PASSWORD

LIBS_FROM_GIT=osprofiler,python-openstackclient

NOVA_REPO=https://review.openstack.org/p/openstack/nova
NOVA_BRANCH=refs/changes/03/254703/39

KEYSTONE_REPO=https://review.openstack.org/p/openstack/keystone
KEYSTONE_BRANCH=refs/changes/35/294535/2

NEUTRON_REPO=https://review.openstack.org/p/openstack/neutron
NEUTRON_BRANCH=refs/changes/51/273951/12

disable_service n-net horizon
enable_service q-svc q-dhcp q-meta q-agt q-l3 neutron

enable_plugin ceilometer https://git.openstack.org/openstack/ceilometer.git
enable_plugin osprofiler https://github.com/openstack/osprofiler.git
#+END_EXAMPLE
- add Fernet tokens usage (Symmetric key encryption) if default token format is still UUID
#+BEGIN_EXAMPLE
KEYSTONE_TOKEN_FORMAT=fernet
#+END_EXAMPLE
- have to have identical cache configuration for Keystone authtoken middleware
  + if cache is external (memcached):
#+BEGIN_EXAMPLE
[keystone_authtoken]
memcache_servers = <memcached_host>:<memcached_port>[,<memcached_host>:<memcached_port>]
signing_dir = <signing_dir>
cafile = <cafile.pem>
auth_uri = <auth_uri>
project_domain_id = <domain>
project_name = <service>
user_domain_id = <domain>
password = <password>
username = <project_user_name>
auth_url = <auth_url>
auth_plugin = <password>
#+END_EXAMPLE

**** Multi node installation

- depends of the chosen OS deployment tool
- OSprofiler, Nova, Neutron and Keystone might have to be patched (links in the plan if necessary)
- enable Ceilometer in the ~event_definitions.yaml~
#+BEGIN_EXAMPLE
- event_type: profiler.*
  traits:
    project:
      fields: payload.project
    service:
      fields: payload.service
    name:
      fields: payload.name
    base_id:
      fields: payload.base_id
    trace_id:
      fields: payload.trace_id
    parent_id:
      fields: payload.parent_id
    timestamp:
      fields: payload.timestamp
    host:
      fields: payload.info.host
    path:
      fields: payload.info.request.path
    query:
      fields: payload.info.request.query
    method:
      fields: payload.info.request.method
    scheme:
      fields: payload.info.request.scheme
    db.statement:
      fields: payload.info.db.statement
    db.params:
      fields: payload.info.db.params
#+END_EXAMPLE
- and for extended tracing informations in the ~ceilometer.conf~
#+BEGIN_EXAMPLE
[event]
store_raw=info
#+END_EXAMPLE
- *beware* either turn off not needed events in the .yaml file or save enough room for the Ceilometer backend storage because every info level event will be stored
- for every service, in their .conf file
#+BEGIN_EXAMPLE
[profiler]
enabled = True
trace_sqlalchemy = True
hmac_keys = SECRET_KEY
#+END_EXAMPLE
- they turned profiling on for Cinder, Glance, Nova, Neutron and Keystone.

**** Test Case 1: Keystone DB / cache operations analysis

- records all HTTP, RPC and DB calls in selected control plane operations
  + includes Keystone operations and their duration (via OSprofiler)
- focused of these control plane operations:
  + Keystone token get (token issue)
  + Keystone user list
  + Keystone endpoint list
  + Keystone service list
  + Nova instance boot (server create)
#+BEGIN_EXAMPLE
openstack --profile SECRET_KEY token issue
openstack --profile SECRET_KEY user list
openstack --profile SECRET_KEY endpoint list
openstack --profile SECRET_KEY service list
openstack --profile SECRET_KEY server create --image <image_id> --flavor <flavor_id> <server_name>
#+END_EXAMPLE
- to initiate OS request tracing =profile <HMAC_KEY>= option needs to be added to the CLI command
  + the key is one of the secret keys define in the .conf
  + tracing of the creation of a VM
#+BEGIN_EXAMPLE
openstack --profile SECRET_KEY server create --image <image> --flavor <flavor> <server-name>
#+END_EXAMPLE
  + at the end of the output, there is a message ~osprofiler trace show...~ to know how to get the trace

- parameters

|-----------------------------+----------------------------------------------------|
| Parameter name              | Value                                              |
|-----------------------------+----------------------------------------------------|
| OpenStack release           | Liberty, Mitaka                                    |
| Cache                       | on, off                                            |
| Token type                  | UUID, fernet                                       |
| Environment characteristics | single node, multi-node (clusterized DB/memcached) |
|-----------------------------+----------------------------------------------------|

- performance metrics
|----------+----------------+-------------------+-------------------------------------------|
| Priority | Value          | Measurement Units | Description                               |
|----------+----------------+-------------------+-------------------------------------------|
|        1 | Operation time | milliseconds      | Time spent on every HTTP/RPC/DB operation |
|----------+----------------+-------------------+-------------------------------------------|

- *beware* OSProfiler wrapper includes time spent on Python operations inside methods. For DB calls tracing OSprofiler uses =before/after_cursor_execute=

**** Test Case 2: Keystone DB / cache operations analysis (HA version)

- adds failover testing component
  + first test run is the same as test case 1
  + second needs to happen after turning off one of the distributed components used by Keystone (example: stop one of memcached instances)

- parameters
|-----------------------------+----------------------------------------------------|
| Parameter name              | Value                                              |
|-----------------------------+----------------------------------------------------|
| OpenStack realease          | Pike                                               |
| Cache                       | on, off                                            |
| Token type                  | UUID, fernet                                       |
| Environment characteristics | single node, multi-node (clusterized DB/memcached) |
| Memcached cluster status    | 3 nodes, 2 nodes, 1 node                           |
| Galera cluster status       | 3 nodes, 2 nodes, 1 node                           |
|-----------------------------+----------------------------------------------------|

- performance metrics
|----------+----------------+-------------------+-------------------------------------------|
| Priority | Value          | Measurement Units | Description                               |
|----------+----------------+-------------------+-------------------------------------------|
|        1 | Operation time | milliseconds      | Time spent on every HTTP/RPC/DB operation |
|----------+----------------+-------------------+-------------------------------------------|



*** SQL Database Test Plan

- what are the best pratices for scale and performance of SQL database deployments in OS while maintaining avaibility or all ACID properties?
- only MySQL and variants
- testing done in isolation from other OS components
- Two parts on this plan:
  1. use sysbench to drive simple queries
  2. real db extracted from a deployed production OS and a set of corresponding queries
     + the queries can be played on the same db with various configurations

- Test environment
  + Cluster of 3 hosts for db tests with replication
  + Tests w/o replication use only 1 of these servers
  + 1 additional client machine
  + Hardware is full documented (processor model and frequency, memory size, storage type and capacity, networking interfaces, etc.)

**** Preparation

- On all 3 DB server hosts (details in the report)
  + MySQL installation
  + MariaDB installation
  + Percona Cluster installation
  + Parameters defined in ~/etc/mysql/my.cnf~
  + Permissions defined for database user
- Description of the environment:
  + hardware, as stated above
  + software
    - which ubuntu
    - =ssh_config=
    - "recent" versions of MySQL, MariaDB, Percona, sysbench (keeping in mind that they use Ubuntu 14.04)
  + sysbench description on 3 hosts
    - examples of sysbench commands used for preparation and run

**** Test case 1: sysbench

- Parameters

|-------------------+-------------------------------|
| Parameter         | Value                         |
|-------------------+-------------------------------|
| Database          | MySQL, MariaDB, Percona       |
| Number of threads | 20, 40, 60, 80, 120, 160, 200 |
| Replication       | 1, 3                          |
|-------------------+-------------------------------|

- Database configurations
  + MySQL/InnoDB with Galera
  + MariaDB/XtraDB with Galera
  + MariaDB/InnoDB with Galera
  + Percona Cluster/XtraDB with Galera
  + MySQL with NDB
  + PostgreSQL

- Performance metrics

|----------+-------------+-------------------+----------------------------------------|
| Priority | Value       | Measurement units | Description                            |
|----------+-------------+-------------------+----------------------------------------|
|        1 | throughput  | tps               | transactions/sec, measured by the tool |
|        1 | query lat   | millisec          | query latency, measured by MySQL       |
|        2 | CPU util    | percent           | Average CPU utilization on db server   |
|        2 | Rx BW       | MB/sec            | Average Network receive bandwidth      |
|        2 | Tx BW       | MB/sec            | Average Network transmit bandwidth     |
|        2 | Read BW     | MB/sec            | Average storage read bandwidth         |
|        2 | Write BW    | MB/sec            | Average storage write bandwidth        |
|        2 | Storage lat | millisec          | Average storage latency                |
|----------+-------------+-------------------+----------------------------------------|

**** Test case 2: Database Testing Tool

- goal : quantify query performance with an real OS DB and corresponding queries to develop a portable tool to test DB.
- uses a backup from Mirantis' 200-node cluster and the corresponding queries, imported in different db
- ultimate goal:
  + which software is best for OpenStack
  + how to best configure database parameters
  + which OpenStack queries consume the most resources and are therefore the best candidates for optimization
- parameters
|-------------------+-------------------------------|
| Parameter         | Value                         |
|-------------------+-------------------------------|
| Database          | MySQL, MariaDB, Percona       |
| Number of threads | 20, 40, 60, 80, 120, 160, 200 |
| Replication       | 1, 3                          |
|-------------------+-------------------------------|

- performance metrics
|----------+-------------+-------------------+----------------------------------------|
| Priority | Value       | Measurement Units | Description                            |
|----------+-------------+-------------------+----------------------------------------|
|        1 | throughput  | tps               | transactions/sec, measured by the tool |
|        1 | query lat   | millisec          | query latency, measured by the tool    |
|        2 | CPU util    | percent           | Average CPU utilization on db server   |
|        2 | Memory util | MB                | Memory used on the server              |
|        2 | Rx BW       | MB/sec            | Average Network receive bandwidth      |
|        2 | Tx BW       | MB/sec            | Average Network transmit bandwidth     |
|        2 | Read BW     | MB/sec            | Average storage read bandwidth         |
|        2 | Write BW    | MB/sec            | Average storage write bandwidth        |
|        2 | Storage lat | millisec          | Average storage latency                |
|----------+-------------+-------------------+----------------------------------------|


*** Our tests

- First proposition
- Goal :: compare Keystone over Cockroach and Keystone over Galera
- parameters

|-----------------------------+-------------------------------|
| Parameter name              | Value                         |
|-----------------------------+-------------------------------|
| OpenStack release           | Pike                          |
| Database                    | MySQL (Galera), CockroachDB   |
| Latency between services    | 1, 10, 100, 1000 ms           |
| Environment characteristics | single node, multi-node       |
| Number of threads           | 20, 40, 60, 80, 120, 160, 200 |
| Replication                 | 1, 3                          |
|-----------------------------+-------------------------------|

- Feels like I have no idea what I'm doing, so I submit the parameters to Ronan
  + He confirms that he's not sure if CockroachDB support threads
    - I found no information regarding multithreading for CockroachDB, so it won't be relevant
  + He also finds that the latencies are not relevant
    - I use [[https://wondernetwork.com/pings/Tokyo][the array he gave me]] to put better value
    - He adds that usually it takes at maximum 400ms to ping the entire world
    - I am not sure whether I should add 500ms or it won't be relevant
    - Removed 500, changed the intervals a bit
    - Added 5 replicates because it is usually recommended

|-----------------------------+-----------------------------|
| Parameter name              | Value                       |
|-----------------------------+-----------------------------|
| OpenStack release           | Pike                        |
| Database                    | MySQL (Galera), CockroachDB |
| Latency between services    | LAN, 10, 50, 100, 300 ms    |
| Environment characteristics | single node, multi-node     |
| Replication                 | 1, 3, 5                     |
|-----------------------------+-----------------------------|

- Have to add number of nodes in the swarm

|-----------------------------+-----------------------------|
| Parameter name              | Value                       |
|-----------------------------+-----------------------------|
| OpenStack release           | Pike                        |
| Database                    | MySQL (Galera), CockroachDB |
| Latency between services    | LAN, 10, 50, 100, 300 ms    |
| Environment characteristics | single node, multi-node     |
| Cluster size                | 1, 5, 25, 100               |
| Replication                 | 1, 3, 5                     |
|-----------------------------+-----------------------------|

** Readings

*** The hows and whys of a distributed SQL database by Alex Robinson
- history
  - first databases were tightly coupled, required a lot of work to use
  - 70s SQL RDBMS queries really independent from the physical storage used for a single machine
  - 80-90s beginning of object-oriented databases tries but not really worked
    + lacked a standard API like SQL
  - early 2000s custom sharding
    + db grew rapidly \to required sharding
    + w/o cross-shard transactions, really hard to operate
  - 2004s NoSQL gave up the relational model, no transactions, no indexes, manual joins
  - 2010s NewSQL /distributed db
    + easier to let developers deal with lack of performance than the lack of transactions
    + attempt to combine the best of both worlds
- how they are built
  - mostly about spanner and cockroachDB
  - data distribution
    + in SQL
      - all data on one server
      - manually shard data across separate DB instances
    + in NoSQL/NewSQL core assumption that you have to cut your data at some point
      - how do you find a particular piece of data
      - how do you divide the data
    + hashing: easy to locate data, but problem: you can do range scans (don't maintain ordering of the data)
      - used in cassandra by default, dynamoDB(amazon)
      - not used in NewSQL
    + order-preserving: put all your data layed out alphatical order and split up into chunks
      - chunks from A to C, etc. (for example)
      - easy to split, efficient scans of the data (range scan) even if data can be on separate nodes
      - no longer have the deterministic function to find where a key is(require additional indexing to find the data)
      - range index to indicate where a piece of data is, don't change too often
      - have to decide when to split (if too big, slow to move data, if too small, the range-index gets to be huge)
    + data-distribution
      - placement: each range is replicated 3 or more times
      - rebalancing when adding a new node for replication
      - recovery: create new copies of the data to have enough replicants
    + data replication
      - keeping copies in sync
	- cold backups: you don't expect them to be up-to-date
	- primary-secondary replication:
	  + one of the copy is the primary takes all of the writes (sometimes all the reads)
	  + the primary send info to the secondary
	  + asynchronous replication
	    - secondary don't receive instantanously data \to failover would loose recent writes
	  + sync..
	    - extra network delays when you're waiting for the writings to get done
	    - what to do when secondary fails?
	  + failovers are really hard to get right
      - keeping copies in sync in NoSQL db
	+ eventually consistent
	+ different methods to keep data consistent, but usually fail (CRDT eventually converge)
      - keeping copies in sync in NewSQL db
	+ uses distributed consensus protocol
	  - Paxos (really hard)
	  - Raft (available openly)
	+ have an odd number of nodes
	+ commit when a majority of nodes comes to a consensus
	+ speaker explains Raft
	  - during failover of a leader before commiting, the write is abandonned
      - consensus in NewSQL db
	+ run one consensus group *per* range of data
    + transactions
      - ACID transactions (Atomic, Consistent, Isolated, Durable)
      - SQL databases
	+ atomiticity bootstrapped off a lower-level atomic primitive: log writes
	  - all mutations part of a transaction get tagged with a transaction
	+ isolation
	  - R/W locks \to require deadlock detection
	  - MVCC timestamp on each row \to versioning of the row
	    + access to historical data
	    + allow long read only transactions to not block new writes
      - NoSQL db
	+ many systems don't offer transaction at all
      - NewSQL db
	+ support traditional ACID semantics
	+ atomicity bootstrapped off distributed consensus
	+ isolation can be handled similarly to single node SQL db
      - CockroachDB
	+ use MVCC
	+ consensus write with number of transaction
	+ when the key for transaction is still present, it's not committed and so reads won't take this key into account
	+ when write conflict happens (ex: same key two times on the same range)
	   - aborting? requires priority to know which one to abort
	   - restart one ?


** DONE Attending #openstack-meeting
   CLOSED: [2018-01-17 mer. 17:02]




* Thursday 18 January [2018-01-18 jeu.]

** Todos before going anywhere

- After discussions with Ronan yesterday evening, we decided we have to think a little further on the experiments.
  + he gave me things to look up:
    1. [[https://gist.github.com/aphyr/0ad3458a1478db97517e7ac2faf2da00][Advice on benchmarking databases]]
    2. His notes about ACID
  + we have to test Keystone over CockroachDB and Keystone over Galera AND tests using sysbench
    - he gave me this [[https://github.com/akopytov/sysbench/issues/180][link]] which indicates that sysbench supports CockroachDB, which will considerably reduce the effort for the tests

*** Advice on benchmarking databases

1. Pick multiple workloads that covers all behaviors in each database.
2. Get experts from each vendor to tune the test and OS \to will be hard :D
3. Common-denominator tests are helpful but the queries will be different from one DB to another
4. Report concurrency throughput, goodput (application level throughput) and latency distribution (which must be kept reasonable for online benchmarks)
5. Benchmarks should take multiple days to run, operating on realistically sized data sets
6. Use real hardware, hot and cold (see [[http://www.ibmbigdatahub.com/blog/your-big-data-hot-warm-or-cold]] )


- There was a [[http://highscalability.com/blog/2015/10/5/your-load-generator-is-probably-lying-to-you-take-the-red-pi.html][link]] in the discussion, talking about load generators results, the highlights were:
  - If you want to hide the truth from someone show them a chart of all normal traffic with one just one bad spike surging into 95 percentile territory.
  - The number one indicator you should never get rid of is the maximum value. That’s not noise, it’s the signal, the rest is noise.
  - 99% of users experience ~99.995%’ile response times, so why are you even looking at 95%'ile numbers?
  - Monitoring tools routinely drop important samples in the result set, leading you to draw really bad conclusions about the quality of the performance of your system.
- I will try to store that in a corner of my mind

- Hopefully I will keeep all of that in mind, but some advice seems really hard to use in practice.

*** ACID notes

- Quick reminder:
  - Atomicity :: a transaction is all or nothing
  - Consistency :: transaction is a transition from a valid state of the database to another
  - Isolation :: concurrent or sequential executions of the transactions result in the same system state
  - Durability :: once a transaction is committed, it's stays so

- The ~serializable~ level of isolation is the strongest one. ~Serializable~ transactions run as if only one transaction were running at a time.
  + bad performance
- The ~snapshot~ level guarantees that all reads made in a transaction will see a consistent snapshot of the database, i.e. the last committed values that existed when the current transaction started.
  + better performance because no blocks but write skews happens

- A write skew appears when two concurrent updates apply on the same range of values and commit without conflicting, but produce a non sens result.

- Supposedly, CockroachDB ensures consistency using Raft algorithm so no client would have a stale read (i.e. he won't read an old value for the piece of data he wanted)



** Still working on performance tests plan

- Goal :: compare Keystone over Cockroach and Keystone over Galera

|-----------------------------+-----------------------------|
| Parameter name              | Value                       |
|-----------------------------+-----------------------------|
| OpenStack release           | Pike                        |
| Database                    | MySQL (Galera), CockroachDB |
| Latency between services    | LAN, 10, 50, 100, 300 ms    |
| Environment characteristics | single node, multi-node     |
| Cluster size                | 1, 5, 25, 100               |
| Replication                 | 1, 3, 5                     |
|-----------------------------+-----------------------------|

** Sysbench

- Trying to get it to work on the vagrant machine deployed using ~openstack-cockroachdb-dev~
  - made a dir in stack's home directory called sysbench and then
    #+BEGIN_SRC
    curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.deb.sh | sudo bash
sudo apt -y install sysbench
    #+END_SRC
    as indicated on the [[https://github.com/akopytov/sysbench][sysbench github]]
  - then tried just the dumb first test I've found (on the issue about sysbench support for cockroachDB)
    #+BEGIN_SRC
    sysbench --db-driver=pgsql --pgsql-host=cockroachdb --pgsql-port=26257 --pgsql-user=stack --table-size=1000000 oltp_read_write prepare
    #+END_SRC
    and got:
    #+BEGIN_EXAMPLE
    sysbench 1.0.12 (using bundled LuaJIT 2.1.0-beta2)

    FATAL: Connection to database failed: could not translate host name "cockroachdb" to address: Name or service not known

    FATAL: `sysbench.cmdline.call_command' function failed: /usr/share/sysbench/oltp_common.lua:83: connection creation failed
    #+END_EXAMPLE

- Probably tried just a bit too fast, going to read a bit more documentation.
- Ronan told me that maybe it was a port problem. After checking with him, the command had a problem with pgsql-port
  + we tried with 127.0.0.1:

  #+BEGIN_EXAMPLE
  stack@contrib-jessie:~/sysbench$ sysbench --db-driver=pgsql --pgsql-host=127.0.0.1 --pgsql-port=26257 --pgsql-user=stack --table-size=1000000 oltp_read_write prepare
sysbench 1.0.12 (using bundled LuaJIT 2.1.0-beta2)

Creating table 'sbtest1'...
FATAL: PQexec() failed: 7 database "sbtest" does not exist
FATAL: failed query was: CREATE TABLE sbtest1(
  id SERIAL,
  k INTEGER DEFAULT '0' NOT NULL,
  c CHAR(120) DEFAULT '' NOT NULL,
  pad CHAR(60) DEFAULT '' NOT NULL,
  PRIMARY KEY (id)
)
FATAL: `sysbench.cmdline.call_command' function failed: /usr/share/sysbench/oltp_common.lua:197: SQL error, errno = 0, state = '3D000': database "sbtest" does not exist
  #+END_EXAMPLE

- Connected to CockroachDB with ~cockroach sql --insecure~
- Created database sbtest ~CREATE DATABASE sbtest~
- Relaunched the command

#+BEGIN_EXAMPLE
  stack@contrib-jessie:~/sysbench$ sysbench --db-driver=pgsql --pgsql-host=127.0.0.1 --pgsql-port=26257 --pgsql-user=root --table-size=1000000 oltp_read_write prepare
sysbench 1.0.12 (using bundled LuaJIT 2.1.0-beta2)

Creating table 'sbtest1'...
Inserting 1000000 records into 'sbtest1'
Creating a secondary index on 'sbtest1'...
stack@contrib-jessie:~/sysbench$
#+END_EXAMPLE

- So, now it's working -thanks to Ronan-, I will check how I can get the results back.
- I just reread the command, saw ~prepare~ and so I figured out I have now to ~run~ the tests. I'm using [[https://github.com/akopytov/sysbench][Sysbench github]] to check the commands, but ~sysbench --help~~ also work.

  #+BEGIN_EXAMPLE
  stack@contrib-jessie:~/sysbench$ sysbench --db-driver=pgsql --pgsql-host=127.0.0.1 --pgsql-port=26257 --pgsql-user=root --table-size=1000000 oltp_read_write run
sysbench 1.0.12 (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            9086
        write:                           697
        other:                           3197
        total:                           12980
    transactions:                        649    (64.80 per sec.)
    queries:                             12980  (1296.05 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          10.0132s
    total number of events:              649

Latency (ms):
         min:                                  9.77
         avg:                                 15.42
         max:                                112.64
         95th percentile:                     21.89
         sum:                              10007.61

Threads fairness:
    events (avg/stddev):           649.0000/0.00
    execution time (avg/stddev):   10.0076/0.00


  #+END_EXAMPLE

- Ronan asks me to to the test for postgresql
- First, I do a cleanup
#+BEGIN_EXAMPLE
stack@contrib-jessie:~/sysbench$ sysbench --db-driver=pgsql --pgsql-host=127.0.0.1 --pgsql-port=26257 --pgsql-user=root --table-size=1000000 oltp_read_write cleanup
sysbench 1.0.12 (using bundled LuaJIT 2.1.0-beta2)

Dropping table 'sbtest1'...

#+END_EXAMPLE

- Then I retry with port 5432, which is the default port for PG.
  #+BEGIN_EXAMPLE
  stack@contrib-jessie:~/sysbench$ sysbench --db-driver=pgsql --pgsql-host=127.0.0.1 --pgsql-port=5432 --pgsql-user=root --table-size=1000000 oltp_read_write prepare
sysbench 1.0.12 (using bundled LuaJIT 2.1.0-beta2)

FATAL: Connection to database failed: FATAL:  database "sbtest" does not exist

FATAL: `sysbench.cmdline.call_command' function failed: /usr/share/sysbench/oltp_common.lua:83: connection creation failed

  #+END_EXAMPLE

- Of course, I didn't create ~sbtest~ on postgresql.
  - to connect and create a db
    #+BEGIN_SRC
    sudo su postgres
    psql
    CREATE DATABASE sbtest;
    #+END_SRC


#+BEGIN_EXAMPLE
stack@contrib-jessie:~/sysbench$ sysbench --db-driver=pgsql --pgsql-host=127.0.0.1 --pgsql-port=5432 --pgsql-user=root --table-size=1000000 oltp_read_write prepare
sysbench 1.0.12 (using bundled LuaJIT 2.1.0-beta2)

Creating table 'sbtest1'...
Inserting 1000000 records into 'sbtest1'
Creating a secondary index on 'sbtest1'...
stack@contrib-jessie:~/sysbench$ sysbench --db-driver=pgsql --pgsql-host=127.0.0.1 --pgsql-port=5432 --pgsql-user=root --table-size=1000000 oltp_read_write run
sysbench 1.0.12 (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            43330
        write:                           12380
        other:                           6190
        total:                           61900
    transactions:                        3095   (309.42 per sec.)
    queries:                             61900  (6188.41 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          10.0008s
    total number of events:              3095

Latency (ms):
         min:                                  2.32
         avg:                                  3.23
         max:                                284.35
         95th percentile:                      3.82
         sum:                               9985.19

Threads fairness:
    events (avg/stddev):           3095.0000/0.00
    execution time (avg/stddev):   9.9852/0.00
#+END_EXAMPLE

- As /expected/, the performance are a lot better on postgreSQL


** Readings

- Other than the article mentioned before


*** CockroachDB documentation about its design

- Uses versioned valued
- For isolation it uses:
  + SI :: Snapshot Isolation
	  + pros: simple to implement highly performant
	  + cons: correct for all but some conditions like write skew  (see ACID notes)
  + SSI :: Serialiable Snapshot Isolation. Default level
	  + pros: no anomalous conditions
	  + cons: a bit more complex, less performant
- No starvation even for long transactions
- Uses candidate timestamps for the transaction (current clock time of the node coordonating the transaction)
  + Candidate timestamps increase when coordinating transaction (*but only for SI*). It never decreases, however.
- Hybrid Logical Clock
  + on each cockroach node
  + timestamps are composed of a physical component (~local time) and a logical one (to distinguish events) \to track causality
  + used to pick a timestamp for a transaction
- Transaction execution flow
  + 2 phases:
    - Had real difficulties to understand the first, but basically, it is preparing the commit
      + Begins with the selection of the range the most involved in the transaction
      + Records the transaction to a reserved area in that range with a state ~PENDING~
      + Writes an intent value for each piece of data used in the transaction (there is a intent flag for when the transaction is committed
      + Stores a transaction id with intent(ed) values
	- enables decisions on the ordering of different transactions with the same timmestamp
      + each nodes returns the timestamp used for the write to the client
      + the client takes the highest timestamp for the commit timestamp
    - Committing the transaction with the commit timestamp
      + SI allows different transactions to be made concurrently as it is
      + SSI restart the transaction if the candidate and the commit timestamps differ
      + Once the transition is committed, the intents flags are removed
- Not taking notes about details about less interesting topics for me
- [[https://github.com/cockroachdb/cockroach/blob/master/pkg/config/zone.proto][Zones]] (might be useful later)
- The rest feels a bit too much for now

*** Vitess: MySQL Sharding

- Stopped it because it does not seem relevant for now.

*** Cockroachdb blog article about Scaling Raft

- [[https://www.cockroachlabs.com/blog/scaling-raft/][link]]
- each range has its own Raft consensus group
  - as a result, each node may be participate in hundred of thousands of consensus groups
  - addressed the problem by doing a MultiRaft
    - seems it has been removed: [[https://github.com/cockroachdb/cockroach/blob/098a7292a69292be01558457464079f87657393b/docs/RFCS/20151213_dismantle_multiraft.md][Dismantle multiraft]]
    - not really removed but the multi part of multiraft has been moved to methods on storage
    - multiraft was too tightly coupled with storage and it created duplicated data
    - since it has been only moved, I will still look how it is working
- ranges are small to improve recovery time in case of failure
  - as a consequence there are a LOT of ranges on one node
  - this leads to a huge amount of traffic between nodes since one node participate in many consensus group
  - if I understood correctly, they kind of synchronized the heartbeats
    - each node is managed as a group (all the ranges he stores are managed as a group)
    - this way, each 'pair' of nodes only exchange their heartbeat at the same time (once per tick), regardless of the number of ranges they have in common

*** Serializable, Lockless, Distributed: Isolation in CockroachDB

- [[https://www.cockroachlabs.com/blog/serializable-lockless-distributed-isolation-cockroachdb/][Serializable, Lockless, Distributed: Isolation in CockroachDB]]

- goes more into details how conflicts are resolved
- read it more quickly to avoid spending too much time on non -at least yet- necessary informations


* Friday 19 January [2018-01-19 ven.]

** Beginning the day

- I think I will have to seriously think about and make the plan for the experiments
  + Looking around what the performance team has done
    - not much more that I've already seen... for every other tests ran, only authenticate was measured
    - maybe something on this: [[https://wiki.openstack.org/wiki/KeystonePerformance][Keystone Performance]] (and there are the [[https://github.com/TristanCacqueray/keystone-perfs][scripts used]] too)
    - [[https://docs.openstack.org/developer/performance-docs/test_plans/control_plane/plan.html#test-case-3-keystone-authentication][here]] there is an exemple of rally scenario configuration to test keystone authentication for Nova


  + Looked at [[https://www.openstack.org/assets/presentation-media/openstackperformance-v4.pdf][redhat slides]]
    - s. 12:
      + some consumers asks for several tokens? (horizon login, horizon image page, nova image-list)
      + no automated garbage collector for token
	- need to use =keystone-manage token_flush= (more on that at the bottom of [[https://docs.openstack.org/keystone/pike/admin/identity-troubleshoot.html][this]] page, but we might do something about that?)
    - nothing much more, it's mainly a presentation about how to enhance OS performance and what is costly
