#+TITLE: Report for : Un intergiciel d’une base de données NewSQL qui considère la localité de l’application cliente
#+AUTHOR: Marie Delavergne


* 1st week
** Monday 15 January [2018-01-15 lun.]

*** Installation

- Solving administrative problems (account creations, etc.)
- Working in office B226

*** Preparations

- Cleaned boookmarks to be able to find them more rapidly
- Created folder and repo github for my work.
  + Added gitignore
  + Added tools
- WARNING : I can't run Vagrant without disabling secure boot


*** Readings

**** Spanner

- 2013
- DB
  + scalable
  + multi-version
  + globally-distributed
  + synchronously-replicated
  + google-made
- Reshard automatically data when the amount of data or the number of servers change
- Migrate automatically data across machines and DC
- Scale up to millions of machines, hundreds of DC and trillions of datarows
- Focus: managing cross-DC replicated data
- Evolved from a versioned key-value store to a temporal multi-version db
  + semi-relational db
  + data store with a commit timestamp
  + configurable garbage collector for old versions
- Replication configurations can be dynamically controlled at a fine grain
- More Spanner features that enable Spanner to support consistent backups, MapReduce executions and atomic schema updates
  + externally consistent reads and write
  + globally-conssistent reads across DB at a given timestamp
- Timestamps reflect serialization order
- Organized as a set of zones (unit of administrative deployment)
  + one zonemaster :: assigns data to spanservers
  + (100->n*1000) spanservers
  + tablet :: data structure (similar to Bigtable tablet abstraction)
  + per-zone location proxies :: used by clients to locate the spanservers assigned to serve their data
- Spanserver
  + serve data to clients
  + responsible for between 100 and 1000 instances of tablet
  + implements a single Paxos state machine on top of each tablet
  + logs every Paxos write twice (one in the tablet's log, once in the Paxos log)
- Transaction manager
  + If the transaction involves only one Paxos group, it can bypass the TM (lock table and Paxos provide transactionality)
  + If it involves more than one Paxos group, the groups' leaders coordinate to perform a two-phase commit
    - coordinator leader chosen and coordinator slaves
- Directory (bucket) : set of contiguous keys that share a common prefix
  + allows applications to control the locality of their data by choosing keys carefully
  + directories can be moved on-the-fly for different reasons (movedir)
- Megastore used by at least 300 Google applications
- Two-phase commit too expensive for performance and availability
- Every table must have an ordered set of one or more primary-key columns
- TrueTime represents time as a TTinterval (interval with bounded time uncertainty)

| Method      | Returns                              |
| TT.now()    | TTinterval: [earliest, latest]       |
| TT.after()  | true if t has definitely passed      |
| TT.before() | true if t has definitely not arrived |

- Two-phase commit can scale to up to 50 participants
- During experimentations on availability, we can see that only kill 'hard' a leader has a real impact on the reads completed
- Related works
  + VoltDB uses NewSQL

*** Meeting

- 14:15 with msimonin

**** Work
- Keystone with specific scenarii
  + tempest has more coverage (especially with the new decorator)


** Thursday 16 January [2018-01-16 mar.]

*** Openstack Cockroach Dev

- Resumed vagrant launched yesterday evening
- Used ~vagrant ssh~ to jump to the deployed machine

**** Running tempest
- Jumped to tempest folder
- Used ~testr init~ and then ~testr run --parallel tempest.scenario~ to run tests from [[https://www.openstack.org/assets/presentation-media/TempestScenarioTests-20140512.pdf][Tempest Scenarios Tests]]
  + Received some errors like these :
#+BEGIN_EXAMPLE
Traceback (most recent call last):
  File "/opt/stack/neutron/neutron/tests/tempest/scenario/test_dvr.py", line 66, in test_vm_reachable_through_compute
    self._check_snat_port_connectivity()
  File "/opt/stack/neutron/neutron/tests/tempest/scenario/test_dvr.py", line 32, in _check_snat_port_connectivity
    self._check_connectivity()
  File "/opt/stack/neutron/neutron/tests/tempest/scenario/test_dvr.py", line 29, in _check_connectivity
    self.keypair['private_key'])
  File "/opt/stack/neutron/neutron/tests/tempest/scenario/base.py", line 232, in check_connectivity
    ssh_client.test_connection_auth()
  File "tempest/lib/common/ssh.py", line 207, in test_connection_auth
    connection = self._get_ssh_connection()
  File "tempest/lib/common/ssh.py", line 121, in _get_ssh_connection
    password=self.password)
tempest.lib.exceptions.SSHTimeout: Connection to the 172.24.4.12 via SSH timed out.
User: cirros, Password: None
#+END_EXAMPLE
#+BEGIN_EXAMPLE
Traceback (most recent call last):
  File "/opt/stack/neutron/neutron/tests/tempest/scenario/test_migration.py", line 200, in test_from_dvr_ha_to_dvr
    after_dvr=True, after_ha=False)
  File "/opt/stack/neutron/neutron/tests/tempest/scenario/test_migration.py", line 114, in _test_migration
    router['id'], before_dvr, before_ha)
  File "/opt/stack/neutron/neutron/tests/tempest/scenario/test_migration.py", line 80, in _wait_until_router_ports_ready
    router_id, const.DEVICE_OWNER_DVR_INTERFACE)
  File "/opt/stack/neutron/neutron/tests/tempest/scenario/test_migration.py", line 64, in _wait_until_port_ready
    timeout=300, sleep=5)
  File "/opt/stack/neutron/neutron/common/utils.py", line 697, in wait_until_true
    raise WaitTimeout("Timed out after %d seconds" % timeout)
neutron.common.utils.WaitTimeout: Timed out after 300 seconds
#+END_EXAMPLE
- Might be because only keystone is supposed to work
- Stopped the run because it was running every tests for every components
- Rerun using ~tempest run --regex tempest.api.identity~
#+BEGIN_EXAMPLE
======
Totals
======
Ran: 143 tests in 134.0000 sec.
 - Passed: 128
 - Skipped: 10
 - Expected Fail: 0
 - Unexpected Success: 0
 - Failed: 5
Sum of execute time for each test: 251.9807 sec.

==============
Worker Balance
==============
 - Worker 0 (31 tests) => 0:01:54.281398
 - Worker 1 (28 tests) => 0:01:08.869696
 - Worker 2 (14 tests) => 0:01:39.594563
 - Worker 3 (24 tests) => 0:02:00.720647
 - Worker 4 (32 tests) => 0:01:53.721540
 - Worker 5 (14 tests) => 0:00:50.875559
#+END_EXAMPLE
- Have to see the logs, which is now in a binary file (journalctl)
- ~sudo journalctl --unit devstack@keytsone.service --since -5m~ to see the logs
- I can navigate using /ERR and n to go to the next
#+BEGIN_EXAMPLE
stack@contrib-jessie:~/tempest$ sudo journalctl --unit devstack@keytsone.service --since -5m
-- Logs begin at Tue 2018-01-16 10:15:51 GMT, end at Tue 2018-01-16 10:32:08 GMT. --
stack@contrib-jessie:~/tempest$
#+END_EXAMPLE

- Modified file in keystone folder ~keystone/identity/backends/sql.py~ :
  + Search for '@', copied ~ @oslo_db_api.wrap_db_retry(retry_on_deadlock=True)~
  + Added to the method that failed 'authenticate' (l. 58)
  + After that, I restart the service using ~sudo systemctl restart devstack@keystone~
  + And check if it is correctly loaded with ~sudo systemctl status devstack@keystone~

#+BEGIN_EXAMPLE
======
Totals
======
Ran: 142 tests in 197.0000 sec.
 - Passed: 132
 - Skipped: 10
 - Expected Fail: 0
 - Unexpected Success: 0
 - Failed: 0
Sum of execute time for each test: 435.4079 sec.

==============
Worker Balance
==============
 - Worker 0 (10 tests) => 0:01:49.532543
 - Worker 1 (17 tests) => 0:02:32.577647
 - Worker 2 (22 tests) => 0:02:36.304883
 - Worker 3 (37 tests) => 0:03:10.918512
 - Worker 4 (23 tests) => 0:02:42.695828
 - Worker 5 (33 tests) => 0:02:41.024460

#+END_EXAMPLE

- When doing my PR, I discovered there was a branch called 'deadlock-retry' so I've checked it:
  + turns out there was different changes on it, and the decorator (wrapper) was used on ~_record_failed_auth~ and ~update_user~
  + but when I reran the tests on this branch I got
#+BEGIN_EXAMPLE
======
Totals
======
Ran: 132 tests in 180.0000 sec.
- Passed: 120
- Skipped: 10
- Expected Fail: 0
- Unexpected Success: 0
- Failed: 2
Sum of execute time for each test: 378.7295 sec.

==============
Worker Balance
==============
- Worker 0 (9 tests) => 0:01:30.253865
- Worker 1 (32 tests) => 0:02:36.105035
- Worker 2 (20 tests) => 0:02:24.938662
- Worker 3 (26 tests) => 0:02:29.701283
- Worker 4 (18 tests) => 0:02:09.142470
- Worker 5 (27 tests) => 0:02:47.248556
#+END_EXAMPLE

- After asking my tutor, turns out that the branch was an old one so I just made my PR to cockroachdb/pike

*** Readings

**** Raft

- Goal is to have a result equivalent to Paxos, but more understandable and easier to learn
- Consensus algorithms "allow a collection of machines to work as a coherent group that can survive the failures of some of its members"
- Most implementations of consensus are based on or influenced by Paxos
- Raft uses techniques to improve understandability
  + decomposition: leader election, log replication, safety
  + state space reduction: reduce the degree of nondeterminism and ways servers can be inconsistent with each other
- New features
  + Strong leader:: stronger form of leadership
  + Leader election:: randomized timers
  + Membership changes:: two different configurations overlap when changing the set of servers in a cluster
- Safety properties formally specified and proven

***** Replicated state machine problem

- Consensus algorithms are used in the context of RSM
  + Replicated State Machines:: state machines in a collection of servers compute identical copies of the same state and continue operating even if some of the servers are down
  + It is used to solve a variety of fault tolerance problems in distributed systems

#+CAPTION: RSM architecture from the Raft article
#+NAME: fig:raft_RSM_architecture
[[images/Raft_RSM_architecture.png]]

- Each server stores a log containing a series of commands executed in order by he state machine associated
  + The commands are in the same order on each log -> same sequence executed on all state machines -> same state computed and same sequence of output (because of determinism)
- The replicated log is kept consistent by the consensus algorithm
- Properties of the algorithm
  + safety under all non-Byzantine conditions including network delays, partitions, packet loss, duplication and re-ordering
  + availability if a majority of servers are operational and communicate with each other and clients. (n/2 +1)
  + consistency of the logs independent of timing
  + speed: for most cases, a given command is completed when a majority of the cluster has responded to a single round of remote procedure calls


***** Strengths and weaknesses of Paxos

- Largely used, at least as a starting point
- Different level of Paxos:
  + single-decree Paxos:: able to reach agreement on a single decision (like a single replicated log entry)
  + multi-Paxos:: multiple instances of the former protocol to facilitate a series of decisions (such as an entire log)
- Ensures safety and liveness
- Supports changes in cluster membership
- Two major drawbacks
  + awfully  difficult to understand
  + no good foundation to make practical implementations
- As a consequence, implementations are usually extremely different from the Paxos theory, especially in terms of architecture
  + time consuming and error-prone
  + worse: since the implementations are too different, the correctness of Paxos can't be verified

***** General approach to understandability

- Raft had to:
  + provide a complete and practical foundation for system building
  + be safe under all conditions
  + be available under typical operating conditions
  + be efficient for common operations
  + be UNDERSTANDABLE
    - by a large audience
    - to make possible the development of extensions
- To make it understandable:
  + each time a choice had to be made, it was always for understandability
  + two techniques were used, as seen previously (decomposition and state space reduction)

***** Raft consensus algorithm

1. Election of a leader
   - if a leader fails or is disconnected, a new leader is elected
   - at any time each server is either:
     + leader:: only one
     + follower:: passive, they respond to requests from leaders and candidates
     + candidate:: used to elect a new leader
2. Give responsibility for managing the replicated log to this leader
3. Leader accepts log entries from clients
4. Leader replicates them to other servers
5. Leader tells the servers when they can apply new log entries to their state machines

- Time is divided into /terms/ of arbitrary length
  + Numbered with consecutive integers; each server stores a current term (increasing monotonically)
  + each term begins with an election where one or more candidates attempt to become leader
  + if an election ends with a split vote, the term ends and a new one begins
  + Serves as a logical clock
    - if 2 servers communicate and don't have the same term value, they take the largest one
    - if a candidate or leader discovers this way that his term has ended, he becomes a follower
    - if the server receives a request with an older term, it rejects the request

#+CAPTION: A -not-so- condensed summary of the Raft consensus algorithm
#+NAME: fig:raft_cons_algo
[[images/Raft_cons_algo]]



***** Raft evaluation
***** Related work


* Readings

** Preliminary
- [[https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf][Spanner: Google’s Globally-Distributed Database]]
- [[https://www.usenix.org/system/files/conference/atc14/atc14-paper-ongaro.pdf][In Search of an Understandable Consensus Algorithm]]
- [[https://github.com/cockroachdb/cockroach/blob/master/docs/design.md][CockroachDB doc]]
- [[https://www.youtube.com/watch?v=6OFeuNy39Qg][The hows and whys of a distributed SQL database]] by Alex Robinson
- [[http://vitess.io/][Vitess site]]
- [[https://beyondtheclouds.github.io/blog/openstack/cockroachdb/2017/12/22/a-poc-of-openstack-keystone-over-cockroachdb.html][BTC blog article about Keystone over CockroachDB]]

** Others
- [[https://en.wikipedia.org/wiki/Paxos_(computer_science)][Paxos]]

* How to

** Org mode

To add a date not added to agenda:
#+BEGIN_SRC
Ctrl-c !
#+END_SRC
To demote current subtree by one level:
#+BEGIN_SRC
Alt-Shift-Left
#+END_SRC


** Using Rally & Tempest

*** Tempest

To run only keystone tests:
#+BEGIN_SRC
tempest run --regex tempest.api.identity
#+END_SRC


** Using devstack

Since it's not using screen anymore, everything is in systemctl.

To restart a service:
#+BEGIN_SRC
sudo systemctl restart devstack@SERVICE_NAME
#+END_SRC
To know the status of a service:
#+BEGIN_SRC
sudo systemctl status devstack@SERVICE_NAME
#+END_SRC


** Other

Cut pages in pdf (from [[https://askubuntu.com/questions/221962/how-can-i-extract-a-page-range-a-part-of-a-pdf][Ask Ubuntu]]):
#+BEGIN_SRC
pdftk full-pdf.pdf cat 12-15 output outfile_p12-15.pdf
#+END_SRC
