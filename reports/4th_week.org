#+TITLE: Report for : Un intergiciel d’une base de données NewSQL qui considère la localité de l’application cliente -- 4th week --
#+AUTHOR: Marie Delavergne

* Monday 5 February [2018-02-05 lun.]

- Been thinking about adding vars in =extra_vars= during the ~prepare~ task but I don't think it would be a good way
- The other solution I thought about was to add vars in the beginning of the sysbench ~deploy~, depending on the 'db' var

- For now, I will concentrate on making everything work and then I will see about making it the proper way

- Added the run to the deployment
#+BEGIN_EXAMPLE
root@parapluie-25:~# docker logs sysbench
sysbench 1.0.7 (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            2646
        write:                           360
        other:                           774
        total:                           3780
    transactions:                        189    (18.83 per sec.)
    queries:                             3780   (376.60 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          10.0351s
    total number of events:              189

Latency (ms):
         min:                                 32.82
         avg:                                 53.08
         max:                                391.95
         95th percentile:                     95.81
         sum:                              10032.55

Threads fairness:
    events (avg/stddev):           189.0000/0.00
    execution time (avg/stddev):   10.0326/0.00
#+END_EXAMPLE

- Now, doing the same with CockroachDB
  + Using [[https://www.cockroachlabs.com/docs/stable/use-the-built-in-sql-client.html][Cockroach doc]]
    #+BEGIN_EXAMPLE
root@parapluie-1:~# docker exec -d cockroachdb-master cockroach sql --insecure --execute="CREATE DATABASE sbtest;"
Error response from daemon: OCI runtime exec failed: exec failed: container_linux.go:296: starting container process caused "exec: \"cockroach\": executable file not found in $PATH": unknown
    #+END_EXAMPLE

** Meeting with Matthieu

For cockroach:
- Know where the request has been

For everything:
- Get disk I/O


** Refactoring with Ronan

- Corrected command [[https://github.com/Marie-Donnie/juice/commit/41526a257ef686958c9ff9429644d7a81c6b8a1c][to create database]]
  + Ok but breaks the other nodes since they don't have a cockroach-master docker
- Did a bit of refactoring to avoid duplicated code [[https://github.com/Marie-Donnie/juice/commit/edaebe2e5b6032a0e54c2eb62b64476f3d91112f][here]]
  + Trying to deploy that version now


* Tuesday 6 February [2018-02-06 mar.]

** Meeting with Ronan and Adrien

- About the tests we are going to do

- Reminder:
  #+BEGIN_QUOTE
- several regions
  + common authentication
  + number of regions: 1, 5, 10, 25, (100?)
  + latency between regions (RTT): LAN, 10, 50, 100, 300
    - 1 common keystone
      + one region has the keystone
      + the others refers to this keystone
      + one database
    - 1 keystone per region with a common state
      + Galera vs CockroachDB
    - 1 keystone per region using K2K federation
  #+END_QUOTE

- Adrien is ok with this, so we can resume our work

** Tests

- When deploying yesterday afternoon, I forgot to add the host ip for sysbench deployment
  - ok
    #+BEGIN_EXAMPLE
root@grisou-40:~# docker logs sysbench
sysbench 1.0.7 (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

SQL statistics:
    queries performed:
        read:                            1204
        write:                           86
        other:                           430
        total:                           1720
    transactions:                        86     (8.56 per sec.)
    queries:                             1720   (171.15 per sec.)
    ignored errors:                      0      (0.00 per sec.)
    reconnects:                          0      (0.00 per sec.)

General statistics:
    total time:                          10.0473s
    total number of events:              86

Latency (ms):
         min:                                 97.29
         avg:                                116.82
         max:                                148.84
         95th percentile:                    142.39
         sum:                              10046.54

Threads fairness:
    events (avg/stddev):           86.0000/0.00
    execution time (avg/stddev):   10.0465/0.00
    #+END_EXAMPLE

** Planning

- We prepared a planning for the work to do:
*** summit deadline
    DEADLINE: <2018-02-08 jeu.>
*** be able to backup the results
    DEADLINE: <2018-02-09 ven.>
*** end of tests
    DEADLINE: <2018-03-30 ven.>
*** summit
    SCHEDULED: <2018-05-21 lun.>

*** Making it work

- To do so, I have to look at how we are going to collect informations about CockroachDB and MariaDB
  + For MariaDB, it will probably be [[https://collectd.org/documentation/manpages/collectd.conf.5.shtml#plugin_mysql][collectd]] ( [[http://ftp.nchu.edu.tw/MySQL/doc/refman/5.0/en/server-status-variables.html][server status variables]] )
  + For CockroachDB, we have several leads:
    - [[https://www.cockroachlabs.com/docs/stable/monitor-cockroachdb-with-prometheus.html][Monitor CockroachDB with Prometheus]]
      + config file setups prometheus so he will scrape the time series metrics of a single, insecure local node every 10 seconds : [[https://github.com/cockroachdb/cockroach/blob/master/monitoring/prometheus.yml][default config file]]
	- I can check for several nodes using =targets=
	- I can also change the interval using =scrape_interval=
    - [[https://www.cockroachlabs.com/docs/stable/show-sessions.html][MySQL API]] (=SHOW <OPTION>=)
      + Session seems promising because it gives the node it is connected to, the queries and the timestamp
	- Can be used with =SHOW SESSIONS= (cluster) or =SHOW LOCAL SESSIONS= (local)
      + =SHOW QUERIES= is even more interesting because
	- it gives informations about:
	  + where the query is executed
	  + the query, ofc
	  + how long it has been running
	  + informations about the client that issued the query (address, application name and user)
        - so I can use this database to track both where it comes from and where it is executed
	- I can also track if the query is distributed using [[https://github.com/cockroachdb/cockroach/blob/master/docs/RFCS/20160421_distributed_sql.md][DistSQL]]
    - Find if there is a rest API [[https://github.com/cockroachdb/cockroach/pull/4844][like that]]
    - Use collectd also

** Make a better juice

- I am going to use docopt, which I already used for [[https://github.com/Marie-Donnie/rally-vagrant][Rally Vagrant]]
  + It takes a bit longer than I anticipated/remembered so I am putting this aside for now (=no_push= folder)
  + I will try it again when everything will be ok

- Testing the "creates" option for command so the creation of the database sbtest won't be done twice when deploying with CockroachDB
  + used
    #+BEGIN_SRC
args:
    creates: /database_created
    #+END_SRC
    - works the first launch
    - didn't worked at second launch
      #+BEGIN_EXAMPLE
      fatal: [grisou-16.nancy.grid5000.fr]: FAILED! => {"changed": true, "cmd": ["docker", "exec", "cockroachdb-grisou-16", "./cockroach", "sql", "--insecure", "--execute=CREATE DATABASE sbtest;"], "delta": "0:00:05.424156", "end": "2018-02-06 16:13:31.697221", "failed": true, "rc": 1, "start": "2018-02-06 16:13:26.273065", "stderr": "Error: pq: database \"sbtest\" already exists\nFailed running \"sql\"", "stderr_lines": ["Error: pq: database \"sbtest\" already exists", "Failed running \"sql\""], "stdout": "", "stdout_lines": []}
      #+END_EXAMPLE
  + tried
    #+BEGIN_SRC
      run_once: True
    #+END_SRC
    - same problem
  + works when creating the file manually... will have to debug this + I will probably need to do the same for the test anyway because the sysbench doesn't like restarts:
    #+BEGIN_EXAMPLE
    root@grisou-34:~# docker logs sysbench
sysbench 1.0.7 (using bundled LuaJIT 2.1.0-beta2)

Running the test with following options:
Number of threads: 1
Initializing random number generator from current time


Initializing worker threads...

Threads started!

FATAL: PQexecPrepared() failed: 7 restart transaction: HandledRetryableTxnError: TransactionRetryError: retry txn (RETRY_POSSIBLE_REPLAY): "sql txn" id=6b1397fd key=/Table/51/1/503120 rw=false pri=0.02027573 iso=SERIALIZABLE stat=PENDING epo=0 ts=1517933446.189282943,1 orig=1517933446.187652172,0 max=1517933446.187652172,0 wto=false rop=false seq=16
FATAL: `thread_run' function failed: /usr/share/sysbench/oltp_common.lua:481: SQL error, errno = 0, state = '40001': restart transaction: HandledRetryableTxnError: TransactionRetryError: retry txn (RETRY_POSSIBLE_REPLAY): "sql txn" id=6b1397fd key=/Table/51/1/503120 rw=false pri=0.02027573 iso=SERIALIZABLE stat=PENDING epo=0 ts=1517933446.189282943,1 orig=1517933446.187652172,0 max=1517933446.187652172,0 wto=false rop=false seq=16
    #+END_EXAMPLE

- Looking to use json format for reports
  + Still haven't figured it out when reading from:
    - [[https://github.com/akopytov/sysbench/blob/master/src/lua/internal/sysbench.lua#L91][The implemented function]]
    - I don't understand how this is called and whether the default is called at all since this format isn't the kind of output I get
    - I still probably have to use [[https://mariadb.com/kb/en/library/sysbench-benchmark-setup/][MariaDB sysbench benchmark setup]] but they use their own lua scripts it seems



* Wednesday 7 February [2018-02-07 mer.]

- MySQL plugin for [[https://collectd.org/documentation/manpages/collectd.conf.5.shtml#plugin_mysql][collectd]] collects information about MySQL network traffic, executed statements, requests, query cache and threads using (from [[http://ftp.nchu.edu.tw/MySQL/doc/refman/5.0/en/server-status-variables.html#statvar_Bytes_received][source]]):
  + =bytes (received/sent)=: the number of bytes received from/sent to all clients
  + =com_<statement>=: number of time each statement (insert/delete, etc.) has been executed. For select, it uses =qcache_hits=
  + =handler_<statement>=: number of time <statement> happened (e.g. read next, prepare, rollback)
  + =qcache_<query/count>=: data relative to the query cache (e.g. inserts, hits, total blocks)
  + =thread_<state>=: number of threads in that state (cached, connected, created, running)

** About juice

- Using =posgtresql_db= module for Ansible instead of the command module to insert a database on CockroachDB
  + first time: works like a charm
    #+BEGIN_EXAMPLE
    root@:26257/> SHOW DATABASES;
+--------------------+
|      Database      |
+--------------------+
| crdb_internal      |
| information_schema |
| pg_catalog         |
| sbtest             |
| system             |
+--------------------+
(5 rows)

Time: 1.965848ms
    #+END_EXAMPLE
  + second time:
    #+BEGIN_EXAMPLE
    TASK [sysbench : Create CockroachDB sbtest database for sysbench] **************************************
fatal: [grisou-16.nancy.grid5000.fr]: FAILED! => {"changed": false, "failed": true, "msg": "Database query failed: unknown function: pg_encoding_to_char()\n"}
    #+END_EXAMPLE
  + tried =encoding: 'UTF-8'= but it didn't help, some I removed it.
  + in any case, best thing to do is probably use =destroy= when redeploying
    - Ronan does not agree because when I will put OpenStack on top of the deployment, it will be much too long
    - So we're going to change the play so it won't produce an error after the first deployment
    - We also did "a bit" of changes to finally use docopt: [[https://github.com/Marie-Donnie/juice/commit/0625cc40b8edd0569d578ccac271a4fa534418f4][commit]]


** Thinking about the proposal for the summit


The Fog/Edge Massively Distributed Clouds (FEMDC) working group is trying to monitor how OpenStack will behave in the context of compute resources close to users. We compare the databases performances on a cluster of MariaDB, a fork of MySQL and CockroachDB, a "cloud-native" NewSQL database.

To see how the Keystone service will handle authentication on a varying number of regions and throughout the variation of latency between those regions, we evaluated the performances over different configurations:
- One centralized Keystone on a single region, handling the others requests
- A replicated Keystone throughout the cluster with a common state, comparing MariaDB Galera Cluster and CockroachDB
- A federated Keystone
This presentation exhibits the methodology, the results and the identification of the possible improvements towards a massively distributed OpenStack.

Every required tool is available to reproduce the experiments on Grid'5000.

*** sources

- [[https://docs.openstack.org/security-guide/identity/federated-keystone.html][federated keystone]]
- [[https://mariadb.com/kb/en/library/what-is-mariadb-galera-cluster/][mariadb galera cluster]]


* Thursday 8 February [2018-02-08 jeu.]

- I am not sure if it is a good or bad thing, but Ronan discovered the [[https://www.cockroachlabs.com/docs/stable/demo-follow-the-workload.html][follow-the-worlkload]] option for CockroachDB, which means the purpose of my internship is void, since they already added an option to consider locality. But at least, it is a good thing for the global work of the discovery initiative
  + Doing the tutorial on the VM I still have from openstack-cockroach-dev
    - dl go
    - install comcast via go
      #+BEGIN_EXAMPLE
      stack@contrib-jessie:/home/vagrant$ go get github.com/tylertreat/comcast
package github.com/tylertreat/comcast: cannot download, $GOPATH not set. For more details see: go help gopath
stack@contrib-jessie:/home/vagrant$ go help gopath
The Go path is used to resolve import statements.
It is implemented by and documented in the go/build package.

The GOPATH environment variable lists places to look for Go code.
On Unix, the value is a colon-separated string.
On Windows, the value is a semicolon-separated string.
On Plan 9, the value is a list.

GOPATH must be set to get, build and install packages outside the
standard Go tree.

Each directory listed in GOPATH must have a prescribed structure:

The src/ directory holds source code.  The path below 'src'
determines the import path or executable name.

The pkg/ directory holds installed package objects.
As in the Go tree, each target operating system and
architecture pair has its own subdirectory of pkg
(pkg/GOOS_GOARCH).

If DIR is a directory listed in the GOPATH, a package with
source in DIR/src/foo/bar can be imported as "foo/bar" and
has its compiled form installed to "DIR/pkg/GOOS_GOARCH/foo/bar.a".

The bin/ directory holds compiled commands.
Each command is named for its source directory, but only
the final element, not the entire path.  That is, the
command with source in DIR/src/foo/quux is installed into
DIR/bin/quux, not DIR/bin/foo/quux.  The foo/ is stripped
so that you can add DIR/bin to your PATH to get at the
installed commands.  If the GOBIN environment variable is
set, commands are installed to the directory it names instead
of DIR/bin.

Here's an example directory layout:

    GOPATH=/home/user/gocode

    /home/user/gocode/
        src/
            foo/
                bar/               (go code in package bar)
                    x.go
                quux/              (go code in package main)
                    y.go
        bin/
            quux                   (installed command)
        pkg/
            linux_amd64/
                foo/
                    bar.a          (installed package object)

Go searches each directory listed in GOPATH to find source code,
but new packages are always downloaded into the first directory
in the list.
      #+END_EXAMPLE
    - solved this using, in the .bashrc and then sourcing it:
      #+BEGIN_EXAMPLE
export GOPATH=$HOME/go
      #+END_EXAMPLE
    - my machine is running really slow so I am going to work on g5k

- Ronan tells me to directly implement the tutorial in juice, so I am searching how to make latency vary on enoslib
  + maybe somewhere around [[https://github.com/BeyondTheClouds/enoslib/blob/master/enoslib/api.py#L366][here]], using the syntax in the doc for the conf file
  + trying to make it work, but I'm wondering:
    - how can I test if the latency is ok for our tests
    - do I put as much regions as I have nodes with only a True/False option or do I give a number of region and I divide nodes between those regions ?




** Proposal

The Fog/Edge Massively Distributed Clouds (FEMDC) working group analyzes how OpenStack manages Fog/Edge Cloud infrastructures. In such infrastructures, handling the identity between regions is challenging because of latency and risk of network partition. The Keystone Identity Service offers different deployment approaches, from a centralized one, to a federation, as well as a replication using database clustering. In this presentation, we are going to compare them, varying the number of regions and latency between those regions. Especially:
- One centralized Keystone handling requests of all regions
- A replicated Keystone using Galera Cluster to synchronize databases in the different regions.
- A replicated Keystone leveraging NewSQL mechanisms. Particularly, CockroachDB, the "cloud-native SQL database"
- A federated Keystone
We present the methodology, results and identification of possible improvements towards a massively distributed OpenStack.



Since 2016, the FEMDC SiG has been investigating how OpenStack could operate Edge Cloud infrastructures. Among the challenges that have been identified by the SiG, dealing with latency issues and network split brains is important for most OpenStack services. In this presentation, we focus on how they impact the Keystone Identity Service. Keystone offers different deployment approaches, from a centralized one, to a federation, as well as a replication using database clustering. By varying the number of regions and latency between those regions, we compare the following deployments:
- One centralized Keystone handling requests of all regions
- A replicated Keystone using Galera Cluster to synchronize databases in the different regions
- A replicated Keystone leveraging the NewSQL CockroachDB database
- A federated Keystone
We present the methodology, results and identification of possible improvements towards a more decentralized management of the OpenStack services. Experimented on Grid'5000.



A comparison of different Keystone deployments in the context of Fog/Edge Cloud Infrastructures


While Fog/Edge Cloud Infrastructures are becoming more recognised, the question of using OpenStack to operate such infrastructures, in particular the performance/scalability of the system, is often debated. In this presentation, we will present a reproducible way to evaluate the performance of OpenStack Keystone at a WAN level.

Attendees may expect to learn:

    scenarios to operate WANwide infrastructures
    how to perform reproducible experiments thanks to the Enoslib framework
    methodology to emulate network limitations in a WANwide deployment based on traffic shapping
    preliminary results of the impact of the latency on the behaviour of Keystone
