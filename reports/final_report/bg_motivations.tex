\section{Edge computing}

More and more applications use the geographic distribution of the users for a better service, such as videos streams, the internet of things related applications, or the autonomous driving applications. This kind of applications are deployed on edge infrastructures. An edge infrastructure can be defined as up to hundreds individually-managed and geo-distributed micro datacenters composed of dozens of servers, though the definitions might vary. This is the definition we, at the Discovery initiative, rely on. Edge computing is then a way to get the applications, data and services away from the centralized core to the other end of a network, closest to the users and directly in contact to ``the physical world''. It is as if there was a small brain close to the sensitive part of a neuron, rather than one centralized far away, which creates delay in response. The expected latency and bandwidth between elements may fluctuate, in particular because networks can be wired or wireless. Disconnections between sites may also occur, leading to network partitioning situations. This kind of edge infrastructure shares common basis with cloud computing, notably in terms of management. Therefore developers and operators (DevOps) of an edge infrastructure expect to find most features that made current cloud solutions successful. Unfortunately, there is currently no resource management system able to deliver all these features for the egde\cite{cherrueau:hal-01812747}.

\section{OpenStack}

To study edge computing, the Discovery initiative analyzes and modifies the infrastructure manager OpenStack. OpenStack is an open-source software composed of many different services to control and manage everything needed to operate a datacenter. The services ranges from managing computing resources (e.g. virtual machines with Nova, containers with Magnum or physical machines with Ironic) to storage resources (e.g. disk volumes with Cinder, object stores with Swift), through network resources (e.g. Neutron), monitoring services (e.g. Ceilometer), benchmarking (e.g. Rally), etc.


In a basic OpenStack deployment, some nodes of the datacenter run OpenStack services (e.g. Nova, Neutron, Cinder, \dots) and form the control plane. These nodes configure other nodes of the datacenter to deliver IaaS/PaaS/SaaS capabilites (such as a VM ran with KVM, a virtual network configured by Linux Bridge, a CEPH volume, \dots) which form the data plane. Thus, whenever a user issues a request to the control plane process it, which  may potentially also affect the data plane in some manner.

OpenStack is one of the major solution to manage private and public cloud computing infrastructures and one that can very well be considered to handle the edge computing. Though it had been developped with the idea of being able to scale, some services, like the relational database it relies on, do not satisfy this idea (a centralized database is a single point of failure). This makes OpenStack unable, for now, to manage hundreds of small, massively distributed, datacenters. Moreover, to deploy and manage edge datacenters, OpenStack have to handle high delays. And then again, other services like the message broker become unusable under this high delays. The Discovery initiative decided to adapt OpenStack to operate simultaneously these hundreds of small edge datacenters, to avoid reinventing the wheel and building a new management system from scratch.

Preliminary studies conducted by the Discovery initiative enabled to identify different OpenStack deployment scenarios for the edge: from a fully centralized control plane, to a fully distributed control plane\cite{cherrueau:hal-01812747}.


\section{OpenStack at the Edge: distribute the relational database}

Every OpenStack service uses a relational database to save its state. This database is a single point of failure, which is a problem to achieve the goal of a massively distributed OpenStack we defined previously, so we need to distribute the database. They are difficult to distribute, especially in a high latency context. Indeed, a distributed database is replicated and distributed on different sites, either fully or for some portions. This means that the RDBMS has to choose which copy will be requested, make sure the effect of updates are reflected on all copies of the data, decide what to do when sites fail during an update. Also, since each sites cannot have instantaneous information of the actions currently made on the other sites, the synchronization of the transactions on multiple sites is really harder\cite{OezsuValduriez2011}. More will be said on the subject in the next chapter.

The Discovery initiative investigates on replacing the relational database by a NewSQL one. Indeed, those RDBMS seek to provaide a capacity to scale on-line transaction processing (OLTP) workloads better than the legacy systems while maintaining ACID guaranties for transactions that were used and formalized in the relational model\cite{DBLP:journals/sigmod/PavloA16}. This will be also detailled in the next chapter. In particular, the initiative paved the way towards a use of CockroachDB, which has been inspired by Google's Spanner\cite{CRDB:HLC}, with OpenStack Keystone.

Keystone is the identity service of OpenStack. It manages the authentication of users and map them to the services they can access, using a catalog of all services deployed in the current configuration. Keystone is interesting because it does not use subqueries when making request to the database, so it is easier to ``plug'' to an other database. Moreover, when wanting to distribute OpenStack in different regions, one can either use Galera, that replicates data over the cluster, or use a distributed NewSQL database such as CockroachDB in order to have all her users and services available across all regions.

Nevertheless, we have to think about how to distribute properly the data across all nodes of the clustered database. In the context of a massively distributed edge computing, we must push the idea to keep the data the closest to the users possible to avoid latency in the requests, and ensure resiliency to disconnections and network partitioning. Thus consider the client locality. Also, we can ask how Keystone will behave in all the different control planes discussed above.
