#+TITLE: Report for : Un intergiciel d’une base de données NewSQL qui considère la localité de l’application cliente -- 15th week --
#+AUTHOR: Marie Delavergne
#+BIBLIOGRAPHY: ../misc/biblio plain

* Analysis

- Thought I had it covered, but I figured out only the labels are changed, not the values
  + I was using ~df.index.set_levels(['3','25','45'], 2, inplace=True)~
#+CAPTION: Graph I had friday
#+NAME: fig:no_good
[[../images/full_analysis_with_ck2_for_3nodes.png]]


- Found out how to fix this, but in any case I didn't push what I've done friday
#+CAPTION: Seems to work now
#+NAME: fig:graph_sorted_nodes
[[../images/graph_sorted_nodes_ok.png]]

- Also removed ~keystone_v3.~ from the name of the actions


* Readings

- Putting that somewhere: [[https://www.cockroachlabs.com/docs/stable/partitioning.html#define-table-partitions-by-list][define table partitions by list for cockroachdb]]
  + the scenario is similar to what we want:
#+BEGIN_EXAMPLE
Consider a global online learning portal, RoachLearn, that has a database containing a table of students across the world. Suppose we have two datacenters: one in the United States and another in Australia. To reduce latency, we want to keep the students' data closer to their locations:

    We want to keep the data of the students located in the United States and Canada in the United States datacenter.
    We want to keep the data of students located in Australia and New Zealand in the Australian datacenter.
#+END_EXAMPLE
  + BUT it requires an enterprise version

** MDCC : Multi-Data Center Consistency ( DBLP:conf/eurosys/KraskaPFMF13 )

- Replication across diverse data centers across the globe is different from replication within a cluster since the delays are significantly higher
- Transactions are really important for database-backed applications
  + traditional 2PC has serious drawbacks in a geo-replicated system (requires a coordinator that can fail)
- asynchronous replications often used in highly-available DB
- systems using Paxos
  + these protocols rely on 2PC and a single master, requiring 2 RTT for clients that are not local not the master
- MDCC is an optimistic commit protocol for transactions with a cost similar to eventually consistent protocols
  + uses a single wide-area message RT in most cases
  + does not require a master
  + has a defaut "read-committed" isolation:
    - atomic durability :: all updates in a transaction eventually persist or none
    - read-committed :: updates form uncommitted transactions are never visible to other transactions
    - no lost updates :: concurrent updates of the same record are resolved if commutative or prevented
    - no atomic visibility :: some updates from successful committed transactions might be visible before all updates become visible


* Juice

- Ronan nicely made a PR to merge Cockroach2 branch into the master

- Changed juice.py to get a default db if none

- Making juice work with a rally burst or normally (and with one rally per group if needed)

- Thinking how I'm going to put a list in the prepare.yml with 'database0', etc.
  + Ronan gave me [[https://docs.ansible.com/ansible/latest/user_guide/intro_patterns.html][the link to patterns]] so I'm trying ~hosts : database*~

- Have to leave but got
#+BEGIN_EXAMPLE
fatal: [paravance-27.rennes.grid5000.fr]: FAILED! => {"msg": "The conditional check 'inventory_hostname == dbmaster_node' failed. The error was: error while evaluating conditional (inventory_hostname == dbmaster_node): 'dict object' has no attribute 'database'\n\nThe error appears to have been in '/home/madelavergne/juice-2/ansible/roles/cockroachdb/tasks/deploy.yml': line 11, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n\n- name: Start the first cockroachdb node\n  ^ here\n"}
#+END_EXAMPLE
(working on juice-2)

- Fixed this and other stuff to make it work
  + everything seems ok, checking network
  + 2 groups (10 nodes)
#+BEGIN_EXAMPLE
        "database0": [
            "ecotype-16.nantes.grid5000.fr",
            "ecotype-10.nantes.grid5000.fr",
            "ecotype-13.nantes.grid5000.fr",
            "ecotype-18.nantes.grid5000.fr",
            "ecotype-14.nantes.grid5000.fr"
        ],
        "database1": [
            "ecotype-2.nantes.grid5000.fr",
            "ecotype-22.nantes.grid5000.fr",
            "ecotype-20.nantes.grid5000.fr",
            "ecotype-9.nantes.grid5000.fr",
            "ecotype-7.nantes.grid5000.fr"
        ],
#+END_EXAMPLE
  + on database0:
    #+BEGIN_EXAMPLE
    root@ecotype-10:~# ip addr
[...]
5: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc htb state UP group default qlen 1000
    link/ether 24:6e:96:6a:d1:fa brd ff:ff:ff:ff:ff:ff
    inet 10.44.1.58/18 brd 10.44.63.255 scope global eth1
       valid_lft forever preferred_lft forever
    inet6 fe80::266e:96ff:fe6a:d1fa/64 scope link
       valid_lft forever preferred_lft forever
    #+END_EXAMPLE
    #+BEGIN_EXAMPLE
    root@ecotype-16:~# ip addr
[...]
5: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc htb state UP group default qlen 1000
    link/ether 24:6e:96:6a:46:c2 brd ff:ff:ff:ff:ff:ff
    inet 10.44.1.64/18 brd 10.44.63.255 scope global eth1
       valid_lft forever preferred_lft forever
    inet6 fe80::266e:96ff:fe6a:46c2/64 scope link
       valid_lft forever preferred_lft forever
    #+END_EXAMPLE
  + on database1:
#+BEGIN_EXAMPLE
root@ecotype-2:~# ip addr
[...]
5: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc htb state UP group default qlen 1000
    link/ether 24:6e:96:6a:d0:4a brd ff:ff:ff:ff:ff:ff
    inet 10.44.1.50/18 brd 10.44.63.255 scope global eth1
       valid_lft forever preferred_lft forever
    inet6 fe80::266e:96ff:fe6a:d04a/64 scope link
       valid_lft forever preferred_lft forever
#+END_EXAMPLE
#+BEGIN_EXAMPLE
root@ecotype-16:~# ping 172.16.193.10
PING 172.16.193.10 (172.16.193.10) 56(84) bytes of data.
64 bytes from 172.16.193.10: icmp_seq=1 ttl=64 time=0.124 ms
64 bytes from 172.16.193.10: icmp_seq=2 ttl=64 time=0.085 ms
64 bytes from 172.16.193.10: icmp_seq=3 ttl=64 time=0.083 ms
64 bytes from 172.16.193.10: icmp_seq=4 ttl=64 time=0.082 ms
--- 172.16.193.10 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3070ms
rtt min/avg/max/mdev = 0.082/0.093/0.124/0.020 ms
root@ecotype-16:~# ping 10.44.1.50
PING 10.44.1.50 (10.44.1.50) 56(84) bytes of data.
64 bytes from 10.44.1.50: icmp_seq=1 ttl=64 time=320 ms
64 bytes from 10.44.1.50: icmp_seq=2 ttl=64 time=320 ms
--- 10.44.1.50 ping statistics ---
3 packets transmitted, 2 received, 33% packet loss, time 1999ms
rtt min/avg/max/mdev = 320.124/320.135/320.146/0.011 ms
root@ecotype-16:~# ping 10.44.1.64
PING 10.44.1.64 (10.44.1.64) 56(84) bytes of data.
64 bytes from 10.44.1.64: icmp_seq=1 ttl=64 time=0.039 ms
64 bytes from 10.44.1.64: icmp_seq=2 ttl=64 time=0.025 ms
--- 10.44.1.64 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1009ms
rtt min/avg/max/mdev = 0.025/0.032/0.039/0.007 ms
#+END_EXAMPLE


* Make the PR for Rally

** Gerrit (notes from Ronan)
Slides of the summit:
[[http://docs.openstack.org/upstream-training/workflow-gerrit.html]]

And the sandbox tutorial:
http://docs.openstack.org/infra/manual/sandbox.html

The =git-review= command submits a git branch to gerrit for review.
The =git-review= command has to be installed on top of git. See
https://git.openstack.org/cgit/openstack-infra/git-review.

Commit messages:
[[https://docs.openstack.org/upstream-training/workflow-commit-message.html]]
[[https://wiki.openstack.org/wiki/GitCommitMessages]]

My general workflow is:
1. Fork the repository from github
2. Add the official OpenStack git repository as one of your remote
  : git remote add os git://git.openstack.org/openstack/kolla-ansible
3. Synchronise yourself with the OpenStack Gerrit
  : git review -s
4. Make changes in a dedicated branch and push into rcherrueau
  github...
5. When it's time to push to Gerrit, update author name and email of
  the commit to match these of the Gerrit account
  #+BEGIN_SRC bash
  git config user.name rcherrueau; git config user.email Ronan-Alexandre.Cherrueau@inria.fr
  git commit --amend --reset-author
  #+END_SRC
6. Finally, push to Gerrit
  : git review

Then, for each patch repeat 4 to 6.

If someone does some comments, you should reply /Done/ to each of his
comments:
1. Open the Gerrit diff page of your commit with comment of the
  reviewer.
2. Click on each of it's comment (the yellow bubble) and click the
  /Done/ blue button. The word /Draft/ should come before your
  comment.
3. Go to the Gerrit commit page and be sure to select the patch
  (/Patch Sets/) where you make some comment. A red bubble should
  appears in the /Patch Sets/ menu to notify you that you have
  pending comments. Then, click the /Reply.../ button to summit all
  your /Done/.

** My work

- Fastidious to make everything work!

- After managing to create an account (yep, needed another one) and make the tutorial for the sandbox (yep, I am THAT bad), I finally made [[https://review.openstack.org/#/c/563949/][my commit]]

- Ok, some build failed, but even worse, I didn't push it to the right repository. It seems that they have forked rally: one for Openstack specifically and one to use as a framework for different platforms and environments. Info is [[http://lists.openstack.org/pipermail/openstack-dev/2018-April/129284.html][here]]

- Seems it will not impact our work, but I still have to check for my PR


* Test plan

- Have to work on that, too. Modified 14th week accordingly

- Worked on a juice branch to make it possible
